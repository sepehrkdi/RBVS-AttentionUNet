{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3jlcG2TZxp8"
      },
      "source": [
        "# **Retinal Blood Vessel Segmentation with Attention U-Net**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZFO89hNaCmH"
      },
      "source": [
        "# 1. Setup and data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvvBRbfDFGzB"
      },
      "source": [
        "\n",
        "*   Connected to Google Drive and downloaded the DRIVE dataset from Kaggle.\n",
        "\n",
        "*   Data Organization: Created a specific directory in Google Drive and copied the downloaded dataset there"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQxmEt9fPRON"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from PIL import Image\n",
        "from google.colab import drive\n",
        "import glob\n",
        "import itertools\n",
        "import kagglehub\n",
        "import shutil\n",
        "import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRsyPorAnB4x"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "import cv2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wK8rkvTrqoAO",
        "outputId": "3abb21f8-2f4a-413f-e18e-6b24864de67d"
      },
      "outputs": [],
      "source": [
        "# Step 1: Mount Google Drive (adjust the path to where your DRIVE dataset is stored)\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6Z1qNVGZ6WK",
        "outputId": "c7c24f97-fb42-4943-d871-0bdedfa1945d"
      },
      "outputs": [],
      "source": [
        "# Download the dataset\n",
        "path = kagglehub.dataset_download(\"zionfuo/drive2004\")\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "# Set target directory in your Google Drive\n",
        "target_dir = \"/content/drive/MyDrive/DRIVE_2004\"\n",
        "\n",
        "# Delete target_dir if it exists\n",
        "if os.path.exists(target_dir):\n",
        "    shutil.rmtree(target_dir)\n",
        "\n",
        "# Recreate the directory\n",
        "os.makedirs(target_dir)\n",
        "\n",
        "# Copy the files to Google Drive\n",
        "shutil.copytree(path, target_dir, dirs_exist_ok=True)\n",
        "print(f\"Dataset copied to: {target_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hI3pBroTHcTW"
      },
      "source": [
        "# 2. Helpers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIdqqHJyFgEg"
      },
      "source": [
        "\n",
        "\n",
        "*   **Format Conversion**: Developed a utility function to convert GIF image files to TIF format, addressing potential compatibility issues with other libraries.\\\n",
        "Functions: `convert_gif_to_tif` (uses `PIL.Image`).\n",
        "\n",
        "*   **Image Loading**: Created a function to load image files, specifically extracting the green channel (though later modified to load as grayscale).\\\n",
        "Functions: `load_image` (uses `cv2.imread`).\n",
        "\n",
        "*   **Mask Loading**: Implemented a function to load mask files and process them into binary representations.\\\n",
        "Functions: `load_mask` (uses `cv2.imread`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mauGh96bZl9a"
      },
      "outputs": [],
      "source": [
        "def convert_gif_to_tif(src_folder, dest_folder):\n",
        "    \"\"\"Convert all .gif files in src_folder to .tif files in dest_folder, then delete the original .gif.\"\"\"\n",
        "    if not os.path.exists(dest_folder):\n",
        "        os.makedirs(dest_folder)\n",
        "    for filename in os.listdir(src_folder):\n",
        "        if filename.lower().endswith('.gif'):\n",
        "            img_path = os.path.join(src_folder, filename)\n",
        "            with Image.open(img_path) as img:\n",
        "                base_name = os.path.splitext(filename)[0]\n",
        "                new_filename = base_name + '.tif'\n",
        "                new_path = os.path.join(dest_folder, new_filename)\n",
        "                img.save(new_path)\n",
        "            # Remove original gif file\n",
        "            os.remove(img_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_CFxPUxhL9-"
      },
      "source": [
        "Maybe I will try it(Actually, I did not since it messed up the input image, biasing towards much more false positives.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GAIu7AebJn21"
      },
      "outputs": [],
      "source": [
        "# create your CLAHE object once\n",
        "# _clahe = cv2.createCLAHE( clipLimit=2.0, tileGridSize=(8,8) )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgFllyWYZv6K"
      },
      "outputs": [],
      "source": [
        "def load_image(filepath):\n",
        "    \"\"\"\n",
        "    Load an image using OpenCV and extract the green channel.\n",
        "    Note: cv2.imread loads images in BGR order.\n",
        "    \"\"\"\n",
        "    # img = cv2.imread(filepath, cv2.IMREAD_COLOR)\n",
        "    img = cv2.imread(filepath, cv2.IMREAD_GRAYSCALE)\n",
        "    if img is None:\n",
        "        raise ValueError(f\"Could not load image: {filepath}\")\n",
        "    # green_channel = img[:, :, 1]  # extract green channel\n",
        "    # green_channel = _clahe.apply(green_channel)\n",
        "    # return green_channel\n",
        "    return img\n",
        "\n",
        "\n",
        "# def load_image(filepath, target_size=(512,512)):\n",
        "#     img = cv2.imread(filepath, cv2.IMREAD_COLOR)\n",
        "#     if img is None:\n",
        "#         raise ValueError(f\"Could not load image: {filepath}\")\n",
        "#     # 1) resize\n",
        "#     img = cv2.resize(img, target_size, interpolation=cv2.INTER_AREA)\n",
        "#     # 2) extract green channel\n",
        "#     green = img[..., 1]\n",
        "#     return green[..., np.newaxis]  # shape=(512,512,1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jt3okCXDac5n"
      },
      "outputs": [],
      "source": [
        "def load_mask(filepath):\n",
        "    img = cv2.imread(filepath, cv2.IMREAD_COLOR)\n",
        "    if img is None:\n",
        "        raise ValueError(f\"Could not load image: {filepath}\")\n",
        "    if img.ndim == 3:\n",
        "        img = img[...,0]\n",
        "    img = (img > 128).astype(np.uint8)\n",
        "    return img[..., np.newaxis]\n",
        "\n",
        "\n",
        "# def load_mask(filepath):\n",
        "#     \"\"\"\n",
        "#     Load a mask using PIL (to support .gif files), convert to a numpy array,\n",
        "#     and threshold it to create a binary mask.\n",
        "#     \"\"\"\n",
        "#     with Image.open(filepath) as img:\n",
        "#         mask = np.array(img)\n",
        "#     # If the mask has 3 channels, take the first channel\n",
        "#     if mask.ndim == 3:\n",
        "#         mask = mask[:, :, 0]\n",
        "#     # Convert to binary mask (assuming values >128 are foreground)\n",
        "#     mask = (mask > 128).astype(np.uint8)\n",
        "#     return mask\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92Vuv8eGHgZo"
      },
      "source": [
        "# 3. Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5DrcEG0G_ce"
      },
      "source": [
        "\n",
        "\n",
        "*   Converted GIF files in the training and test mask directories to TIF format.\\\n",
        "`convert_gif_to_tif`() was called on `train_mask_dir`, and `test_mask_dir`.\n",
        "\n",
        "*   Tested loading a sample training image and its corresponding mask.\\\n",
        "`load_image()`, `load_mask()`, and\n",
        "`plt.imshow()` was used to display the loaded images.\n",
        "The output shows the shapes of the loaded training image `(584, 565)` and mask `(584, 565, 1)`, confirming successful loading.\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "        \n",
        "    \n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PidaICxJZnXz"
      },
      "outputs": [],
      "source": [
        "train_image_dir = \"/content/drive/MyDrive/DRIVE_2004/DRIVE/training/images/\"\n",
        "train_mask_dir  = \"/content/drive/MyDrive/DRIVE_2004/DRIVE/training/1st_manual/\"\n",
        "\n",
        "test_image_dir = \"/content/drive/MyDrive/DRIVE_2004/DRIVE/test/images/\"\n",
        "test_mask_dir  = \"/content/drive/MyDrive/DRIVE_2004/DRIVE/test/1st_manual/\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Vpfku6lrn5L"
      },
      "outputs": [],
      "source": [
        "# Since OpenCV is not good with gif files\n",
        "convert_gif_to_tif(train_mask_dir, train_mask_dir)\n",
        "convert_gif_to_tif(test_mask_dir, test_mask_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "id": "poVVr9hwaLkL",
        "outputId": "518fac76-f177-45b5-ddcb-99a3a505ff74"
      },
      "outputs": [],
      "source": [
        "# Test loading one image\n",
        "train_image_path = train_image_dir + '22_training.tif'\n",
        "train_image = load_image(train_image_path)\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.imshow(train_image, cmap='gray')\n",
        "plt.title(\"Training Image Sample\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "train_image.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "id": "bpSwl5cHadpN",
        "outputId": "789996f9-19f9-4166-ef65-b6f4db7d6c9a"
      },
      "outputs": [],
      "source": [
        "# Test loading one manual annotation mask\n",
        "mask_path = train_mask_dir + '22_manual1.tif'\n",
        "mask = load_mask(mask_path)\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.imshow(mask, cmap='gray')\n",
        "plt.title(\"Manual Annotation Mask\")\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "mask.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkt78xSVHkQ5"
      },
      "source": [
        "# 4. U-Net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjZ0E7sCH4bM"
      },
      "source": [
        "\n",
        "\n",
        "*   Defined `double_conv_block` function for two convolutional layers with ReLU activation.\\\n",
        "`layers.Conv2D()` with `kernel_size=3`, `padding=\"same\"`, `activation=\"relu\"`\n",
        "\n",
        "*   Defined `downsample_block` function which applies the double convolution block, followed by MaxPooling and Dropout.\\\n",
        "`layers.MaxPooling2D(pool_size=(2, 2))`, `layers.Dropout(0.2)`\n",
        "\n",
        "*   Defined `upsample_block` function for the decoder path which performs transposed convolution, concatenates with skip features, and applies the double convolution block.\\\n",
        "`layers.Conv2DTranspose()`, `layers.concatenate()`\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "        \n",
        "    \n",
        "        \n",
        "    \n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZjyEzwgteKx"
      },
      "outputs": [],
      "source": [
        "def double_conv_block(x, n_filters):\n",
        "  # builds two convolutional layers, with n_filters.  Let's use a filter size 3x3\n",
        "  x = layers.Conv2D(n_filters, kernel_size=3, padding=\"same\", activation=\"relu\")(x)\n",
        "  x = layers.Conv2D(n_filters, kernel_size=3, padding=\"same\", activation=\"relu\")(x)\n",
        "  return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqpIXbk2vqO8"
      },
      "outputs": [],
      "source": [
        "def downsample_block(x, n_filters):\n",
        "  f = double_conv_block(x, n_filters)\n",
        "  p = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(f)\n",
        "  ###### The second most effective thing, adding the dropout after seeing overfitting\n",
        "  p = layers.Dropout(0.2)(p)\n",
        "  return f, p  # return both for skip connection (f) and pooling output (p)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBMwXgSawcSj"
      },
      "outputs": [],
      "source": [
        "def upsample_block(x, conv_features, n_filters):\n",
        "    # Step 1: Upsample the input feature map by a factor of 2 using Conv2DTranspose\n",
        "    # - kernel_size = 3\n",
        "    # - strides = 2 (to double the spatial dimensions)\n",
        "    # - padding = \"same\" to maintain output size compatibility\n",
        "    x = layers.Conv2DTranspose(n_filters, kernel_size=3, strides=2, padding=\"same\")(x)\n",
        "\n",
        "    # Step 2: Concatenate the upsampled feature map with the corresponding encoder feature map (skip connection)\n",
        "    # - axis=-1 to concatenate along the channel axis\n",
        "    x = layers.concatenate([x, conv_features], axis=-1)\n",
        "\n",
        "    # Step 3: Apply two convolutional layers using the double_conv_block function\n",
        "    x = double_conv_block(x, n_filters)\n",
        "\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "om3dU_W9wuTu"
      },
      "outputs": [],
      "source": [
        "def Unet():\n",
        "  # Input layer\n",
        "  inputs = layers.Input(shape=(128, 128, 1))   # 1 channel\n",
        "\n",
        "  # Encoder - Contracting Path\n",
        "  f1, p1 = downsample_block(inputs, 64)   # 128 -> 64\n",
        "  f2, p2 = downsample_block(p1, 128)      # 64 -> 32\n",
        "  f3, p3 = downsample_block(p2, 256)      # 32 -> 16\n",
        "  f4, p4 = downsample_block(p3, 512)      # 16 -> 8\n",
        "\n",
        "  # Bottleneck\n",
        "  b = double_conv_block(p4, 1024)         # 8x8\n",
        "\n",
        "  # Decoder - Expanding Path\n",
        "  u6 = upsample_block(b, f4, 512)         # 8 -> 16\n",
        "  u7 = upsample_block(u6, f3, 256)        # 16 -> 32\n",
        "  u8 = upsample_block(u7, f2, 128)        # 32 -> 64\n",
        "  u9 = upsample_block(u8, f1, 64)         # 64 -> 128\n",
        "\n",
        "  # Output layer: softmax for multi-class segmentation (3 classes in this example)\n",
        "  outputs = layers.Conv2D(1, 1, activation=\"sigmoid\")(u9)\n",
        "\n",
        "  return tf.keras.Model(inputs, outputs, name=\"U-Net\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVVXE5YBAFAm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNuT0aCj3phf"
      },
      "source": [
        "# 5. Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIXlTGX8MRo9"
      },
      "source": [
        "\n",
        "\n",
        "*   Implemented `gating_signal` for the attention mechanism.\\\n",
        "\n",
        "    `layers.Conv2D()`, `layers.BatchNormalization()`, `layers.Activation('relu')`\n",
        "\n",
        "\n",
        "*   Implemented `attention_block` to apply additive attention to filter skip connections\\\n",
        "\n",
        "    `layers.Conv2D()`, `layers.Conv2DTranspose()`, `layers.add()`, `layers.Activation('relu')`, `layers.Conv2D(1, ...)` (for attention coefficients), `layers.UpSampling2D()`, `layers.Lambda()` (for tiling), `layers.multiply()`, `layers.Conv2D(..., 1, ...)` (for final projection), `layers.BatchNormalization()`\n",
        "\n",
        "\n",
        "*   Modified the `upsample_block` to incorporate the attention mechanism by using `gating_signal` and `attention_block`.\\\n",
        "\n",
        "\n",
        "*   Defined the `Unet` model incorporating the attention blocks in the decoder path.\\\n",
        "\n",
        "    *    `layers.Input()`\n",
        "    *    Sequential application of `downsample_block`, `double_conv_block` (bottleneck), and `upsample_block`.\n",
        "    *    Final `layers.Conv2D(1, 1, activation=\"sigmoid\")` for the output layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAkm_4X53rYO"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "# --------------------------------------------------\n",
        "#   Base convolutional block: two 3x3 conv layers\n",
        "# --------------------------------------------------\n",
        "def double_conv_block(x, n_filters):\n",
        "    x = layers.Conv2D(n_filters, kernel_size=3, padding=\"same\", activation=\"relu\")(x)\n",
        "    x = layers.Conv2D(n_filters, kernel_size=3, padding=\"same\", activation=\"relu\")(x)\n",
        "    return x\n",
        "\n",
        "# --------------------------------------------------\n",
        "#   Attention components\n",
        "# --------------------------------------------------\n",
        "\n",
        "def gating_signal(x, out_size):\n",
        "    '''1x1 conv + BN + ReLU to project decoder feature map as gating signal.'''\n",
        "    g = layers.Conv2D(out_size, kernel_size=1, padding='same')(x)\n",
        "    g = layers.BatchNormalization()(g)\n",
        "    g = layers.Activation('relu')(g)\n",
        "    return g\n",
        "\n",
        "\n",
        "def attention_block(x, gating, inter_channels):\n",
        "    '''Additive attention gate to filter skip-connection features.'''\n",
        "    # Project encoder features (x) down\n",
        "    theta_x = layers.Conv2D(inter_channels, kernel_size=2, strides=2, padding='same')(x)\n",
        "\n",
        "    # Project gating signal\n",
        "    phi_g = layers.Conv2D(inter_channels, kernel_size=1, padding='same')(gating)\n",
        "    # Upsample gating to match theta_x's spatial dimensions\n",
        "    shape_theta = K.int_shape(theta_x)\n",
        "    shape_phi = K.int_shape(phi_g)\n",
        "    up_phi = layers.Conv2DTranspose(\n",
        "        inter_channels,\n",
        "        kernel_size=3,\n",
        "        strides=(shape_theta[1] // shape_phi[1], shape_theta[2] // shape_phi[2]),\n",
        "        padding='same'\n",
        "    )(phi_g)\n",
        "\n",
        "    # Fuse and activate\n",
        "    concat_xg = layers.add([up_phi, theta_x])\n",
        "    act_xg = layers.Activation('relu')(concat_xg)\n",
        "\n",
        "    # Generate attention coefficients\n",
        "    psi = layers.Conv2D(1, kernel_size=1, padding='same')(act_xg)\n",
        "    sigmoid_xg = layers.Activation('sigmoid')(psi)\n",
        "\n",
        "    # Upsample coefficients to original encoder size\n",
        "    shape_sig = K.int_shape(sigmoid_xg)\n",
        "    shape_x = K.int_shape(x)\n",
        "    up_psi = layers.UpSampling2D(\n",
        "        size=(shape_x[1] // shape_sig[1], shape_x[2] // shape_sig[2])\n",
        "    )(sigmoid_xg)\n",
        "\n",
        "    # Tile across channels\n",
        "    up_psi = layers.Lambda(\n",
        "        lambda z, rep: K.repeat_elements(z, rep, axis=3),\n",
        "        arguments={'rep': shape_x[3]}\n",
        "    )(up_psi)\n",
        "\n",
        "    # Apply attention to encoder features\n",
        "    y = layers.multiply([up_psi, x])\n",
        "\n",
        "    # Final linear projection\n",
        "    result = layers.Conv2D(shape_x[3], kernel_size=1, padding='same')(y)\n",
        "    result = layers.BatchNormalization()(result)\n",
        "    return result\n",
        "\n",
        "# --------------------------------------------------\n",
        "#   Downsampling block remains unchanged\n",
        "# --------------------------------------------------\n",
        "\n",
        "def downsample_block(x, n_filters):\n",
        "    f = double_conv_block(x, n_filters)\n",
        "    p = layers.MaxPooling2D(pool_size=(2, 2))(f)\n",
        "    p = layers.Dropout(0.2)(p)\n",
        "    return f, p\n",
        "\n",
        "# --------------------------------------------------\n",
        "#   Upsampling block with attention\n",
        "# --------------------------------------------------\n",
        "\n",
        "def upsample_block(x, conv_features, n_filters):\n",
        "    # 1) Compute gating signal from pre-upsample decoder features\n",
        "    g = gating_signal(x, n_filters)\n",
        "    # 2) Upsample the decoder feature map\n",
        "    x = layers.Conv2DTranspose(n_filters, kernel_size=3, strides=2, padding=\"same\")(x)\n",
        "    # 3) Apply attention gate on skip features\n",
        "    attn_feats = attention_block(conv_features, g, inter_channels=n_filters)\n",
        "    # 4) Concatenate and apply double convolution\n",
        "    x = layers.concatenate([x, attn_feats], axis=-1)\n",
        "    x = double_conv_block(x, n_filters)\n",
        "    return x\n",
        "\n",
        "# --------------------------------------------------\n",
        "#   U-Net with Attention Gates\n",
        "# --------------------------------------------------\n",
        "\n",
        "def Unet(input_shape=(128, 128, 1)):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Encoder\n",
        "    f1, p1 = downsample_block(inputs, 64)\n",
        "    f2, p2 = downsample_block(p1, 128)\n",
        "    f3, p3 = downsample_block(p2, 256)\n",
        "    f4, p4 = downsample_block(p3, 512)\n",
        "\n",
        "    # Bottleneck\n",
        "    b = double_conv_block(p4, 1024)\n",
        "\n",
        "    # Decoder with attention at each skip connection\n",
        "    u6 = upsample_block(b, f4, 512)\n",
        "    u7 = upsample_block(u6, f3, 256)\n",
        "    u8 = upsample_block(u7, f2, 128)\n",
        "    u9 = upsample_block(u8, f1, 64)\n",
        "\n",
        "    # Output layer\n",
        "    outputs = layers.Conv2D(1, kernel_size=1, activation=\"sigmoid\")(u9)\n",
        "\n",
        "    model = models.Model(inputs, outputs, name=\"Attention_U-Net\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QL9PgWhNHpCr"
      },
      "source": [
        "# 6. Patches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbDts7ecQaIK"
      },
      "source": [
        "\n",
        "\n",
        "*   Using patches to preserve thin vessel structures and increase training samples. **Since the vessel annotation is so thin. The information gets distorted and sometimes lost during the resizing**. Rather than warping the whole image, we can extract many 128x128 overlapping patches from my 584×565 frames. That means:\n",
        "\n",
        "  *    No global resize → vessel shapes stay true.\n",
        "\n",
        "  *    We get more training samples.\n",
        "\n",
        "  *    During inference we tile-and-stitch back together.\n",
        "\n",
        "\n",
        "\n",
        "*   Defined `extract_patches` function to extract overlapping patches from full-size images and masks.\n",
        "\n",
        "  *    Uses nested loops to slide a window of `patch_size` with a specified `stride`.\n",
        "  *    Uses `np.stack()` to combine patches into a single array.\n",
        "\n",
        "*   Defined `load_and_prep_image` and `load_and_prep_mask` to load full-resolution images and masks and ensure they have a channel dimension.\n",
        "  *   `load_image()`, `load_mask() `\n",
        "  *   `np.newaxis` to add a channel dimension\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eooumWDyqD-M"
      },
      "outputs": [],
      "source": [
        "def extract_patches(img, mask, patch_size=128, stride=8):\n",
        "    H, W = img.shape[:2]\n",
        "    patches_img, patches_msk = [], []\n",
        "\n",
        "    # Slide window\n",
        "    for y in range(0, H - patch_size + 1, stride):\n",
        "        for x in range(0, W - patch_size + 1, stride):\n",
        "            patch_img = img[y:y+patch_size, x:x+patch_size]\n",
        "            patch_msk = mask[y:y+patch_size, x:x+patch_size]\n",
        "\n",
        "            # sanity check: both should be exactly patch_size²\n",
        "            assert patch_img.shape == (patch_size, patch_size, 1)\n",
        "            assert patch_msk.shape == (patch_size, patch_size, 1)\n",
        "\n",
        "            patches_img.append(patch_img)\n",
        "            patches_msk.append(patch_msk)\n",
        "\n",
        "    # Stack into big array: shape = (num_patches,128,128,1)\n",
        "    return np.stack(patches_img, axis=0), np.stack(patches_msk, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JyDNqZYOtjz-"
      },
      "outputs": [],
      "source": [
        "def load_and_prep_image(fp):\n",
        "    img = load_image(fp)             # e.g. shape=(584,565)\n",
        "    if img.ndim == 2:\n",
        "        img = img[..., np.newaxis]   # now (584,565,1)\n",
        "    return img.astype(\"uint8\")       # keep uint8 until normalization later\n",
        "\n",
        "def load_and_prep_mask(fp):\n",
        "    msk = load_mask(fp)              # e.g. shape=(584,565)\n",
        "    if msk.ndim == 2:\n",
        "        msk = msk[..., np.newaxis]   # now (584,565,1)\n",
        "    return msk                        # uint8 {0,1}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c08iol0IHwHQ"
      },
      "source": [
        "# 7. Train and Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJq3t0wRv_3F"
      },
      "source": [
        "\n",
        "*   **Data Preparation**: Loaded file paths for training images and masks and then extracted patches from all training data using the defined `extract_patches` function.\n",
        "\n",
        "\n",
        "*   **Data Formatting**: Concatenated all extracted patches and normalized the image data.\n",
        "  *    Outputs: Printed the shape of the final training set, indicating a significant number of patches (e.g., (63800, 128, 128, 1)).\n",
        "\n",
        "*   **Sample Visualization**: Implemented and used a function to display pairs of image patches and their corresponding masks, visually validating the patch extraction.\n",
        "  *    Functions: `show_sample` (uses `matplotlib.pyplot`).\n",
        "  *    Outputs: Displayed sample patches.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJT4JO2Xs6f6",
        "outputId": "1d031791-a470-41c6-b2fb-a8a4204a7594"
      },
      "outputs": [],
      "source": [
        "# 1. Gather your file‐lists (sorted so patches line up with masks)\n",
        "train_imgs = sorted(glob.glob(os.path.join(train_image_dir, \"*_training.tif\")))\n",
        "train_msks = sorted(glob.glob(os.path.join(train_mask_dir,  \"*_manual1.tif\")))\n",
        "\n",
        "all_X, all_y = [], []\n",
        "\n",
        "for img_fp, msk_fp in zip(train_imgs, train_msks):\n",
        "    # a) load full-res image + mask\n",
        "    img = load_and_prep_image(img_fp)    # (584,565,1)\n",
        "    msk = load_and_prep_mask(msk_fp)     # (584,565,1)\n",
        "\n",
        "    # b) extract overlapping patches\n",
        "    Xp, yp = extract_patches(img, msk, patch_size=128, stride=8)\n",
        "    #    Xp.shape = (n_patches_image, 128,128,1)\n",
        "    #    yp.shape = same\n",
        "\n",
        "    all_X.append(Xp)\n",
        "    all_y.append(yp)\n",
        "\n",
        "# 2. Concatenate along the “example” axis\n",
        "X_train = np.concatenate(all_X, axis=0)   # shape = (total_patches,128,128,1)\n",
        "y_train = np.concatenate(all_y, axis=0)   # shape = (total_patches,128,128,1)\n",
        "\n",
        "# 3. Convert to float32 & normalize images\n",
        "X_train = X_train.astype(\"float32\") / 255.0\n",
        "y_train = y_train.astype(\"float32\")       # keep 0 or 1\n",
        "\n",
        "print(\"Final training set:\", X_train.shape, y_train.shape)\n",
        "# e.g. (1600,128,128,1) if you got 100 patches per image × 16 images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rhoY2k4xu_vz",
        "outputId": "dcfccf8f-fc76-42ce-aea8-ed190058abfa"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def show_sample(X, y, index=0):\n",
        "    \"\"\"Visualize one input patch and its corresponding mask.\"\"\"\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(6, 3))\n",
        "    axs[0].imshow(X[index].squeeze(), cmap='gray')\n",
        "    axs[0].set_title(\"Input Image\")\n",
        "    axs[0].axis(\"off\")\n",
        "\n",
        "    axs[1].imshow(y[index].squeeze(), cmap='gray')\n",
        "    axs[1].set_title(\"Mask\")\n",
        "    axs[1].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example: show the 0th patch\n",
        "for i in range(10):\n",
        "  show_sample(X_train, y_train,\n",
        "              index= random.randint(0, len(X_train)),\n",
        "              # index = i,\n",
        "              )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAn3PMr7H1DM"
      },
      "source": [
        "# 8. Loss function and training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCwYUd2wyqBL"
      },
      "source": [
        "1. Loss components\n",
        "\n",
        "    Implemented **Binary Focal Loss**.\n",
        "        FL = − α y (1−p)^γ log p − (1−α)(1−y) p^γ log(1−p)\n",
        "        α = 0.9 gives vessels 9× weight; γ = 7 forces focus on hard pixels.\n",
        "    Implemented **Dice coefficient** and **Dice loss (1 − Dice)**.\n",
        "        Dice = (2 |Y∩Ŷ| + ε) / (|Y| + |Ŷ| + ε)\n",
        "        Balances classes and rewards global overlap.\n",
        "\n",
        "2. Combined loss\n",
        "\n",
        "    Added the two terms.\n",
        "        loss = Focal(α = 0.9, γ = 7) + (1 − Dice)\n",
        "        Pixel-level hardness + structure-level overlap share the same optimum.\n",
        "\n",
        "3. Training dynamics\n",
        "\n",
        "    Early epochs: Focal dominates.\n",
        "    Mid-late epochs: Dice ramps up; closes gaps and enforces continuity.\n",
        "    Convergence: both terms → 0 only when every vessel pixel is captured and background is clean.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKkf6nUUwsrd"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "_EPS = 1e-6\n",
        "_SMOOTH = 1e-6\n",
        "\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "# 2) Binary Focal Loss (fixed shape handling)\n",
        "# ─────────────────────────────────────────────────────────────\n",
        "\n",
        "def binary_focal_loss(alpha=0.25, gamma=2.0, from_logits=False):\n",
        "    \"\"\"\n",
        "    FL = - alpha * y_true * (1-p)^γ * log(p)\n",
        "         - (1-alpha)* (1-y_true) * p^γ * log(1-p)\n",
        "\n",
        "    Returns a callable to pass into model.compile.\n",
        "    \"\"\"\n",
        "    def loss_fn(y_true, y_pred):\n",
        "        # If logits, convert to probabilities:\n",
        "        if from_logits:\n",
        "            y_pred = tf.sigmoid(y_pred)\n",
        "\n",
        "        # Clip to avoid log(0) instabilities\n",
        "        y_pred = tf.clip_by_value(y_pred, _EPS, 1.0 - _EPS)\n",
        "\n",
        "        # compute cross-entropy per-pixel\n",
        "        ce = - (y_true * tf.math.log(y_pred) +\n",
        "                (1.0 - y_true) * tf.math.log(1.0 - y_pred))\n",
        "        # p_t = p if y_true=1 else (1-p)\n",
        "        p_t = tf.where(tf.equal(y_true, 1.0), y_pred, 1.0 - y_pred)\n",
        "\n",
        "        # focal weight\n",
        "        alpha_factor = tf.where(tf.equal(y_true, 1.0),\n",
        "                                alpha,\n",
        "                                1.0 - alpha)\n",
        "        modulating_factor = tf.pow(1.0 - p_t, gamma)\n",
        "\n",
        "        # combine\n",
        "        loss = alpha_factor * modulating_factor * ce\n",
        "\n",
        "        # average over all pixels & batch\n",
        "        return tf.reduce_mean(loss)\n",
        "\n",
        "    return loss_fn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y60TXAKd6nX-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmppvLS1DX2k"
      },
      "outputs": [],
      "source": [
        "# Combined: focal + dice\n",
        "def dice_coef(y_true, y_pred):\n",
        "    y_true_f = tf.reshape(y_true, [-1])\n",
        "    y_pred_f = tf.reshape(y_pred, [-1])\n",
        "    inter = tf.reduce_sum(y_true_f * y_pred_f)\n",
        "    return (2.*inter + _SMOOTH)/(tf.reduce_sum(y_true_f)+tf.reduce_sum(y_pred_f)+_SMOOTH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "by9Pudc31tnm"
      },
      "source": [
        "\n",
        "\n",
        "*   **Defined Keras Callbacks**: `ModelCheckpoint` to save the best model based on validation accuracy and `EarlyStopping` to stop training when validation accuracy stops improving\n",
        "\n",
        "  *   `tf.keras.callbacks.ModelCheckpoint(...)`\n",
        "  *   `tf.keras.callbacks.EarlyStopping(...)`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulwVrZLM-XkS"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# 1. ModelCheckpoint: save only the best model on validation loss\n",
        "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath='checkpoints/best_model.h5',\n",
        "    monitor='val_accuracy',\n",
        "    save_best_only=True,\n",
        "    mode='max',\n",
        "    save_weights_only=False,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 2. EarlyStopping: stop when val_loss stops improving\n",
        "earlystop_cb = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    patience=3,\n",
        "    mode='max',\n",
        "    restore_best_weights=True,\n",
        "    verbose=1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSQ-iSDn2Dlc"
      },
      "source": [
        "Compiled the model using the Adam optimizer and a combined loss function (Binary Focal Loss + Dice Loss)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oHM7dCTO8-J3",
        "outputId": "b2bd04f3-c421-4e3c-bc53-611ad3aaf29a"
      },
      "outputs": [],
      "source": [
        "unet = Unet()\n",
        "\n",
        "# wnetloss=binary_focal_loss(alpha=0.25, gamma=2.0)\n",
        "unet.compile(\n",
        "  optimizer=\"adam\",\n",
        "  # loss='binary_crossentropy',\n",
        "  # loss=binary_focal_loss(alpha=0.9, gamma=5),\n",
        "  loss=lambda yt, yp: binary_focal_loss(alpha=0.9, gamma=7.0)(yt, yp) + (1.-dice_coef(yt, yp)),\n",
        "  # loss=binary_focal_loss(alpha=0.75, gamma=2.5),\n",
        "  metrics=['accuracy']\n",
        ")\n",
        "unet.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4v5G0Fl2K3S"
      },
      "source": [
        "The training output shows metrics (accuracy and loss) for each epoch on both the training and validation sets. It also shows when the model checkpoint is saved (when `val_accuracy` improves). The model trained for 20 epochs, and validation accuracy generally improved over time, reaching a peak of 0.97764."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IoFAOg0d_xQp",
        "outputId": "bc9583d1-9598-4b78-f9da-4e4cb7d384de"
      },
      "outputs": [],
      "source": [
        "# unet.fit(\n",
        "#     X_train, y_train,\n",
        "#     validation_split=0.05,\n",
        "#     batch_size=8,\n",
        "#     epochs=5,\n",
        "#     shuffle=True\n",
        "# )\n",
        "\n",
        "unet.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    validation_split=0.01,\n",
        "    # batch_size=4,\n",
        "    batch_size=8,\n",
        "    epochs=20,\n",
        "    shuffle=True,\n",
        "    callbacks=[checkpoint_cb]\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntXI3G_wH5Yw"
      },
      "source": [
        "# 9. Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frS8ZauD2WOF"
      },
      "source": [
        "In order to run the model by yourself, you should download the h5 file in the /content/drive/MyDrive directory and mount your google drive on colab. Below I did as such."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bE4Zdh8zfCED",
        "outputId": "c7db661f-4a26-4e84-e064-02b7516e1141"
      },
      "outputs": [],
      "source": [
        "!ls /content/drive/MyDrive/ | grep 'best_model'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "4_r53e7igB2Y",
        "outputId": "5b80e15c-05d6-4c68-b501-144163269d6b"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download('checkpoints/best_model.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "83Vjvmd72e_P",
        "outputId": "d9fb9478-6757-49a6-f714-e9da4adb0634"
      },
      "outputs": [],
      "source": [
        "shutil.move('checkpoints/best_model.h5', '/content/drive/MyDrive/best_model_Unet_RBVS.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RF9TscIyiJa8"
      },
      "outputs": [],
      "source": [
        "unet = Unet()  # rebuild architecture exactly\n",
        "unet.load_weights('/content/drive/MyDrive/best_model_Unet_RBVS.h5')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ON3JYCQVw44_"
      },
      "source": [
        "I did not save the history of my training and I had to plot it using the log"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "NOCkQJ61h5PK",
        "outputId": "47c7dba1-8561-4461-9665-81ac58ae2964"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Manually parsed data from the log\n",
        "epochs = list(range(1, 21))\n",
        "accuracy = [0.9401, 0.9784, 0.9851, 0.9878, 0.9893, 0.9904, 0.9912, 0.9917, 0.9921, 0.9924, 0.9927, 0.9929, 0.9931, 0.9933, 0.9935, 0.9936, 0.9937, 0.9938, 0.9939, 0.9940]\n",
        "val_accuracy = [0.9695, 0.9734, 0.9737, 0.9698, 0.9757, 0.9729, 0.9726, 0.9742, 0.9758, 0.9759, 0.9766, 0.9770, 0.9770, 0.9773, 0.9766, 0.9758, 0.9773, 0.9768, 0.9758, 0.9776]\n",
        "loss = [0.3007, 0.1259, 0.0880, 0.0727, 0.0647, 0.0578, 0.0541, 0.0509, 0.0485, 0.0466, 0.0450, 0.0438, 0.0425, 0.0413, 0.0404, 0.0394, 0.0389, 0.0383, 0.0377, 0.0374]\n",
        "val_loss = [0.1870, 0.1868, 0.2021, 0.3013, 0.2054, 0.2670, 0.2703, 0.2444, 0.2195, 0.2305, 0.2055, 0.1951, 0.2019, 0.2007, 0.1958, 0.2233, 0.1783, 0.2048, 0.2235, 0.1840]\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, accuracy, label='Train Accuracy')\n",
        "plt.plot(epochs, val_accuracy, label='Val Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy over Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, loss, label='Train Loss')\n",
        "plt.plot(epochs, val_loss, label='Val Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss over Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlP3tsOph9Pv"
      },
      "outputs": [],
      "source": [
        "plot_history(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMRx2aye7CRa"
      },
      "source": [
        "\n",
        "\n",
        "*   Defined `predict_full` function to predict the segmentation mask for a full-size image by extracting patches, running inference on each patch, and stitching the results back together\n",
        "  *    Handles padding using `np.pad(..., mode=\"reflect\")`.\n",
        "  *    Uses nested loops to iterate through patch locations.\n",
        "  *    `model.predict()` on individual patches.\n",
        "  *    Accumulates predictions in `sum_probs` and `count_map`.\n",
        "  *    Normalizes accumulated probabilities by `count_map`.\n",
        "  *    Crops the result back to the original image size.\n",
        "  *    Applies a `threshold` to get a binary mask.(not needed after a few iterations since the confidence of my model got higher and higher. At the end, I set it to 0.99)\n",
        "  \n",
        "\n",
        "*   Defined `predict_full_debug` which is similar to `predict_full` but also returns a list of individual patch predictions for debugging.\n",
        "\n",
        "*   Defined `show_patches` function to visualize individual image patches, their predicted probability maps, and the corresponding ground truth patches side-by-side.\n",
        "\n",
        "\n",
        "*   Selected a test image and its ground truth mask (image index 18).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLsXW5OcDX0O"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def predict_full(image,\n",
        "                 model,\n",
        "                 patch_size=128,\n",
        "                #  patch_size=64,\n",
        "                 stride=8,\n",
        "                 threshold=0.5):\n",
        "    \"\"\"\n",
        "    image: H×W×1 NumPy array (dtype uint8 [0–255] or float32 [0–1])\n",
        "    model: your trained U-Net\n",
        "    Returns: binary mask H×W uint8 array\n",
        "    \"\"\"\n",
        "    # 1) Ensure float32 [0,1]\n",
        "    img = image.astype(\"float32\")\n",
        "    if img.max() > 1.0:\n",
        "        img /= 255.0\n",
        "\n",
        "    H, W = img.shape[:2]\n",
        "\n",
        "    # 2) Compute padding so that (H_pad - patch_size) % stride == 0\n",
        "    #    and similarly for W. This ensures an integer number of steps.\n",
        "    n_steps_h = math.ceil((H - patch_size) / stride) + 1\n",
        "    n_steps_w = math.ceil((W - patch_size) / stride) + 1\n",
        "    H_pad = (n_steps_h - 1) * stride + patch_size\n",
        "    W_pad = (n_steps_w - 1) * stride + patch_size\n",
        "\n",
        "    pad_h = H_pad - H\n",
        "    pad_w = W_pad - W\n",
        "\n",
        "    # Use “reflect” to avoid zero-border artifacts\n",
        "    img_p = np.pad(img,\n",
        "                   ((0, pad_h), (0, pad_w), (0,0)),\n",
        "                   mode=\"reflect\")\n",
        "\n",
        "    # 3) Prepare accumulators\n",
        "    sum_probs = np.zeros((H_pad, W_pad), dtype=np.float32)\n",
        "    count_map = np.zeros((H_pad, W_pad), dtype=np.uint8)\n",
        "\n",
        "    # 4) Slide over patches\n",
        "    for y in range(0, H_pad - patch_size + 1, stride):\n",
        "        for x in range(0, W_pad - patch_size + 1, stride):\n",
        "            patch = img_p[y:y+patch_size, x:x+patch_size, :]\n",
        "            # Add batch dimension\n",
        "            patch_in = np.expand_dims(patch, axis=0)  # shape (1,128,128,1)\n",
        "            prob = model.predict(patch_in, verbose=0)[0, ..., 0]\n",
        "\n",
        "            # preds is a list: [array_of_out1, array_of_out2]\n",
        "            # prob = model.predict(patch_in, verbose=0)[1][0, ..., 0] # pick the second output, first batch, last channel\n",
        "            # prob shape: (128,128), values in [0,1]\n",
        "\n",
        "\n",
        "            # Accumulate\n",
        "            sum_probs[y:y+patch_size, x:x+patch_size] += prob\n",
        "            count_map[y:y+patch_size, x:x+patch_size] += 1\n",
        "\n",
        "    # 5) Normalize by the number of times each pixel was covered\n",
        "    avg_probs = sum_probs / count_map\n",
        "\n",
        "    # 6) Crop back to original size\n",
        "    avg_probs = avg_probs[:H, :W]\n",
        "\n",
        "    # 7) Threshold to binary mask\n",
        "    bin_mask = (avg_probs >= threshold).astype(np.uint8)\n",
        "    return bin_mask\n",
        "\n",
        "# ——————————————————————————————————————————————————————————————\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rsHjQbMPi6s"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def predict_full_debug(image,\n",
        "                       model,\n",
        "                       patch_size=128,\n",
        "                       stride=64,\n",
        "                       threshold=0.5):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      bin_mask: H×W uint8 array (aggregated & thresholded)\n",
        "      patch_preds: list of tuples (y, x, prob_map) for each patch\n",
        "    \"\"\"\n",
        "    # --- prep ---\n",
        "    img = image.astype(\"float32\")\n",
        "    if img.max() > 1.0:\n",
        "        img /= 255.0\n",
        "    H, W = img.shape[:2]\n",
        "\n",
        "    # compute padded dims\n",
        "    n_h = math.ceil((H - patch_size) / stride) + 1\n",
        "    n_w = math.ceil((W - patch_size) / stride) + 1\n",
        "    H_pad = (n_h - 1)*stride + patch_size\n",
        "    W_pad = (n_w - 1)*stride + patch_size\n",
        "    pad_h, pad_w = H_pad - H, W_pad - W\n",
        "    img_p = np.pad(img, ((0,pad_h),(0,pad_w),(0,0)), mode=\"reflect\")\n",
        "\n",
        "    # accumulators for full‐image fusion\n",
        "    sum_probs = np.zeros((H_pad, W_pad), dtype=np.float32)\n",
        "    count_map = np.zeros((H_pad, W_pad), dtype=np.uint16)\n",
        "\n",
        "    # debug list\n",
        "    patch_preds = []\n",
        "\n",
        "    # --- slide & predict ---\n",
        "    for i, y in enumerate(range(0, H_pad - patch_size + 1, stride)):\n",
        "        for j, x in enumerate(range(0, W_pad - patch_size + 1, stride)):\n",
        "            patch = img_p[y:y+patch_size, x:x+patch_size, :]\n",
        "            prob_map = model.predict(patch[None,...], verbose=0)[0, ..., 0]\n",
        "\n",
        "            # store for debug\n",
        "            patch_preds.append((y, x, prob_map.copy()))\n",
        "\n",
        "            # fuse into full\n",
        "            sum_probs[y:y+patch_size, x:x+patch_size] += prob_map\n",
        "            count_map[y:y+patch_size, x:x+patch_size] += 1\n",
        "\n",
        "    # normalize & crop\n",
        "    avg_probs = sum_probs / count_map\n",
        "    avg_probs = avg_probs[:H, :W]\n",
        "\n",
        "    # final mask\n",
        "    bin_mask = (avg_probs >= threshold).astype(np.uint8)\n",
        "    return bin_mask, patch_preds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJh4ll1dPkPh"
      },
      "outputs": [],
      "source": [
        "def show_patches(img, patches, gt_mask,\n",
        "                 patch_size=128, n_patches=4, threshold=0.5):\n",
        "    \"\"\"\n",
        "    img         : H×W×1 array\n",
        "    patches     : list of tuples where [0]=y, [1]=x, [-1]=prob_map\n",
        "    gt_mask     : full-size H×W binary mask\n",
        "    patch_size  : int\n",
        "    n_patches   : how many rows to plot\n",
        "    threshold   : contour level on prob_map\n",
        "    \"\"\"\n",
        "    n = min(n_patches, len(patches))\n",
        "    fig, axes = plt.subplots(n, 3, figsize=(12, 3*n))\n",
        "\n",
        "    for i in range(n):\n",
        "        y, x = int(patches[i][0]), int(patches[i][1])\n",
        "        prob = patches[i][-1]\n",
        "\n",
        "        # Raw\n",
        "        ax = axes[i,0]\n",
        "        ax.imshow(img[y:y+patch_size, x:x+patch_size, 0], cmap='gray')\n",
        "        ax.set_title(f'Raw @ ({y},{x})')\n",
        "        ax.axis('off')\n",
        "\n",
        "        # Prediction + contour\n",
        "        ax = axes[i,1]\n",
        "        im = ax.imshow(prob, cmap='viridis', vmin=0, vmax=1)\n",
        "        ax.contour(prob >= threshold, colors='r', linewidths=0.5)\n",
        "        ax.set_title('Prediction')\n",
        "        ax.axis('off')\n",
        "\n",
        "        # Ground truth\n",
        "        gt_patch = gt_mask[y:y+patch_size, x:x+patch_size]\n",
        "        ax = axes[i,2]\n",
        "        ax.imshow(gt_patch, cmap='gray')\n",
        "        ax.set_title('Ground Truth')\n",
        "        ax.axis('off')\n",
        "\n",
        "    fig.colorbar(im, ax=axes[:,1].tolist(),\n",
        "                 shrink=0.6, label='P(fg)')\n",
        "    # plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FkHWo6DeCeS"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXf24MnKeCAa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dKU3ebReAdN",
        "outputId": "b40263f1-0165-4dc8-b15c-0606e9d45a0d"
      },
      "outputs": [],
      "source": [
        "len(test_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6lj0p-sxuG9v"
      },
      "outputs": [],
      "source": [
        "# 1. File lists (must be sorted so image[i] matches mask[i])\n",
        "test_images = sorted(glob.glob(os.path.join(test_image_dir, \"*_test.tif\")))\n",
        "test_masks  = sorted(glob.glob(os.path.join(test_mask_dir,  \"*_manual1.tif\")))\n",
        "\n",
        "# 2. Pick the i’th case (here 0)\n",
        "img_fp  = test_images[18]\n",
        "msk_fp  = test_masks[18]\n",
        "\n",
        "# 3. Load & prep\n",
        "raw         = load_image(img_fp)           # (584,565)\n",
        "# clahe_raw   = _clahe.apply(raw)            # apply CLAHE\n",
        "test_img    = raw[..., np.newaxis]   # (584,565,1)\n",
        "\n",
        "\n",
        "pred_mask   = predict_full(test_img,\n",
        "                           unet,\n",
        "                          #  wnet,\n",
        "                           patch_size=128,\n",
        "                          #  patch_size=64,\n",
        "                           stride=16,\n",
        "                           threshold=0.999)\n",
        "\n",
        "gt_mask     = load_mask(msk_fp)            # (584,565), binary {0,1}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "id": "12ROxTjQ259r",
        "outputId": "da3d7aaa-6e9a-49d0-bf8e-ebf35e2756df"
      },
      "outputs": [],
      "source": [
        "# 1. File lists (must be sorted so image[i] matches mask[i])\n",
        "test_images = sorted(glob.glob(os.path.join(test_image_dir, \"*_test.tif\")))\n",
        "test_masks  = sorted(glob.glob(os.path.join(test_mask_dir,  \"*_manual1.tif\")))\n",
        "\n",
        "# 2. Pick the i’th case (here 0)\n",
        "img_fp  = test_images[18]\n",
        "msk_fp  = test_masks[18]\n",
        "\n",
        "# 3. Load & prep\n",
        "raw         = load_image(img_fp)           # (584,565)\n",
        "# clahe_raw   = _clahe.apply(raw)            # apply CLAHE\n",
        "test_img    = raw[..., np.newaxis]   # (584,565,1)\n",
        "\n",
        "\n",
        "pred_mask   = predict_full(test_img,\n",
        "                           unet,\n",
        "                          #  wnet,\n",
        "                           patch_size=128,\n",
        "                          #  patch_size=64,\n",
        "                           stride=128,\n",
        "                           threshold=0.9)\n",
        "\n",
        "gt_mask     = load_mask(msk_fp)            # (584,565), binary {0,1}\n",
        "\n",
        "# 4. Plot side‐by‐side\n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "plt.subplot(1,3,1)\n",
        "plt.title(\"Green Channel\")\n",
        "plt.imshow(test_img[...,0], cmap='gray')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1,3,2)\n",
        "plt.title(\"Predicted Mask\")\n",
        "plt.imshow(pred_mask, cmap='gray')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1,3,3)\n",
        "plt.title(\"Ground-Truth Mask\")\n",
        "plt.imshow(gt_mask, cmap='gray')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2qCiKJpmQ_kX",
        "outputId": "52bb4a02-c378-419a-c8ff-98b128783675"
      },
      "outputs": [],
      "source": [
        "# 1. collect file paths\n",
        "test_images = sorted(glob.glob(os.path.join(test_image_dir, \"*_test.tif\")))\n",
        "test_masks  = sorted(glob.glob(os.path.join(test_mask_dir,  \"*_manual1.tif\")))\n",
        "\n",
        "# 2. pick an index\n",
        "idx = 18\n",
        "img_fp, msk_fp = test_images[idx], test_masks[idx]\n",
        "\n",
        "# 3. load data\n",
        "raw      = load_image(img_fp)           # (H, W)\n",
        "test_img = raw[..., np.newaxis]         # (H, W, 1)\n",
        "gt_mask  = load_mask(msk_fp)            # (H, W) binary\n",
        "\n",
        "# 4. run your debug-predict\n",
        "#    note: threshold here only affects the *aggregated* full mask,\n",
        "#          patch_preds always hold raw prob maps.\n",
        "bin_mask, patch_preds = predict_full_debug(\n",
        "    test_img, unet,\n",
        "    patch_size=128,\n",
        "    stride=64,\n",
        "    threshold=0.999\n",
        ")\n",
        "\n",
        "# 5. show the first 6 patches\n",
        "show_patches(\n",
        "    test_img,\n",
        "    patch_preds,     # <-- unpacked list of (y,x,prob_map)\n",
        "    gt_mask,\n",
        "    patch_size=128,\n",
        "    n_patches=30,\n",
        "    threshold=0.999\n",
        ")\n",
        "\n",
        "# # 6. if you also want the full-image side-by-side:\n",
        "# plt.figure(figsize=(12,5))\n",
        "# for i,(arr,title) in enumerate([\n",
        "#     (test_img[...,0],   \"Raw Image\"),\n",
        "#     (bin_mask,          \"Aggregated Prediction\"),\n",
        "#     (gt_mask,           \"Ground Truth\")\n",
        "# ]):\n",
        "#     ax = plt.subplot(1,3,i+1)\n",
        "#     ax.imshow(arr, cmap='gray')\n",
        "#     ax.set_title(title); ax.axis('off')\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7fjeT-g8stD"
      },
      "source": [
        "Displayed an overlay of the predicted mask contour (green) and ground truth mask contour (red) on the original test image (test image 18, stride 128, threshold 0.9 result)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "pLill6uOF9g9",
        "outputId": "808fb68f-d16b-4099-a62a-605026c68080"
      },
      "outputs": [],
      "source": [
        "plt.imshow(test_img[...,0], cmap='gray')\n",
        "plt.contour(gt_mask[...,0],  colors='r', linewidths=1)   # GT in green\n",
        "plt.contour(pred_mask, colors='g', linewidths=1)   # Pred in red\n",
        "plt.title(\"GT (green) vs Pred (red)\")\n",
        "plt.axis('off')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLjND9Ebhlt8"
      },
      "source": [
        "# 10. Results\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qfmchyrm9CsG"
      },
      "source": [
        "\n",
        "\n",
        "*   Defined a `compute_metrics` function to calculate Accuracy, IoU, and F1 Score for binary masks\n",
        "  *    Uses NumPy operations on flattened arrays.\n",
        "  *    Includes `_SMOOTH` for numerical stability.\n",
        "\n",
        "*   Iterated through all 20 test images.\n",
        "  *    Loaded each image and its corresponding ground truth mask.\n",
        "  *    Generated a predicted mask using `predict_full` with `patch_size=128`, `stride=64`, and `threshold=0.9`.\n",
        "  *    Computed Accuracy, IoU, and F1 Score for each image using `compute_metrics`.\n",
        "  *    Printed the metrics for each image.\n",
        "  *    Displayed the side-by-side visualization (Image, Predicted Mask, Ground Truth Mask) and the contour overlay (GT red, Pred green) for each image.\n",
        "  *    \n",
        "\n",
        "*   Computed and printed the average Accuracy, IoU, and F1 Score across all test images.\n",
        "  *    Average Accuracy: 0.9639\n",
        "  *    Average IoU: 0.6383\n",
        "  *    Average F1 Score: 0.7787\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPEPlqrIuhrs"
      },
      "outputs": [],
      "source": [
        "pred_mask = pred_mask.squeeze()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsA98ovQul9m"
      },
      "outputs": [],
      "source": [
        "gt_mask = gt_mask.squeeze()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "k12wsFEJDXsu",
        "outputId": "4d51d213-ae4b-40d6-f45f-1cb78c8ae374"
      },
      "outputs": [],
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Compute Accuracy, IoU, and F1 Score for binary masks.\n",
        "    y_true, y_pred: numpy arrays of the same shape, with binary values (0 or 1)\n",
        "    \"\"\"\n",
        "    _SMOOTH = 1e-7  # for numerical stability\n",
        "\n",
        "    y_true_f = y_true.flatten()\n",
        "    y_pred_f = y_pred.flatten()\n",
        "\n",
        "    # Accuracy\n",
        "    correct_pixels = np.sum(y_true_f == y_pred_f)\n",
        "    total_pixels = len(y_true_f)\n",
        "    accuracy = correct_pixels / total_pixels\n",
        "\n",
        "    # IoU (Jaccard Index)\n",
        "    intersection = np.sum(y_true_f * y_pred_f)\n",
        "    union = np.sum(y_true_f) + np.sum(y_pred_f) - intersection\n",
        "    iou = (intersection + _SMOOTH) / (union + _SMOOTH)\n",
        "\n",
        "    # F1 Score (Dice Coefficient)\n",
        "    f1 = (2 * intersection + _SMOOTH) / (np.sum(y_true_f) + np.sum(y_pred_f) + _SMOOTH)\n",
        "\n",
        "    return accuracy, iou, f1\n",
        "\n",
        "\n",
        "# 1. Collect file paths\n",
        "test_images = sorted(glob.glob(os.path.join(test_image_dir, \"*_test.tif\")))\n",
        "test_masks  = sorted(glob.glob(os.path.join(test_mask_dir,  \"*_manual1.tif\")))\n",
        "\n",
        "all_accuracies = []\n",
        "all_ious = []\n",
        "all_f1s = []\n",
        "\n",
        "# 2. Iterate through all test images\n",
        "for i in range(len(test_images)):\n",
        "    img_fp  = test_images[i]\n",
        "    msk_fp  = test_masks[i]\n",
        "\n",
        "    # 3. Load & prep\n",
        "    raw         = load_image(img_fp)\n",
        "    test_img    = raw[..., np.newaxis]\n",
        "\n",
        "    # 4. Predict the mask\n",
        "    pred_mask   = predict_full(test_img,\n",
        "                               unet,\n",
        "                               patch_size=128,\n",
        "                               stride=64,\n",
        "                               threshold=0.5)\n",
        "\n",
        "    pred_mask = pred_mask.squeeze()\n",
        "\n",
        "    gt_mask     = load_mask(msk_fp)\n",
        "    gt_mask = (gt_mask > 0).astype(np.uint8)\n",
        "    gt_mask = gt_mask.squeeze()\n",
        "\n",
        "    # 5. Compute metrics for the current image\n",
        "    accuracy, iou, f1 = compute_metrics(gt_mask, pred_mask)\n",
        "    all_accuracies.append(accuracy)\n",
        "    all_ious.append(iou)\n",
        "    all_f1s.append(f1)\n",
        "\n",
        "    print(f\"Image {i+1}/{len(test_images)}: Accuracy = {accuracy:.4f}, IoU = {iou:.4f}, F1 = {f1:.4f}\")\n",
        "\n",
        "    # 6. Show visualizations for the current image\n",
        "    plt.figure(figsize=(15,5))\n",
        "    plt.subplot(1,3,1)\n",
        "    plt.title(f\"Image {i+1}: Green Channel\")\n",
        "    plt.imshow(test_img[...,0], cmap='gray')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1,3,2)\n",
        "    plt.title(f\"Image {i+1}: Predicted Mask\")\n",
        "    plt.imshow(pred_mask, cmap='gray')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1,3,3)\n",
        "    plt.title(f\"Image {i+1}: Ground-Truth Mask\")\n",
        "    plt.imshow(gt_mask, cmap='gray')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(6,6))\n",
        "    plt.imshow(test_img[...,0], cmap='gray')\n",
        "    plt.contour(gt_mask,  colors='r', linewidths=1)   # GT in red\n",
        "    plt.contour(pred_mask, colors='g', linewidths=1)  # Pred in green\n",
        "    plt.title(f\"Image {i+1}: GT (red) vs Pred (green)\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# 7. Compute and print average metrics\n",
        "avg_accuracy = np.mean(all_accuracies)\n",
        "avg_iou = np.mean(all_ious)\n",
        "avg_f1 = np.mean(all_f1s)\n",
        "\n",
        "print(\"\\n--- Overall Average Metrics ---\")\n",
        "print(f\"Average Accuracy across test images: {avg_accuracy:.4f}\")\n",
        "print(f\"Average IoU across test images: {avg_iou:.4f}\")\n",
        "print(f\"Average F1 Score across test images: {avg_f1:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "JZFO89hNaCmH",
        "hI3pBroTHcTW",
        "92Vuv8eGHgZo"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
